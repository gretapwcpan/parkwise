# Parkwise Unified LLM Service Configuration
# Choose one of three scenarios below

# ==================================================
# SCENARIO 1: OpenAI-Compatible API (Cloud or Local)
# ==================================================
LLM_MODE=api

# For OpenAI
API_BASE_URL=https://api.openai.com/v1
API_KEY=sk-your-openai-key-here
API_MODEL=gpt-4-turbo
API_TEMPERATURE=0.3
API_MAX_TOKENS=500

# For Azure OpenAI
# API_BASE_URL=https://your-resource.openai.azure.com/openai/deployments/your-deployment
# API_KEY=your-azure-key
# API_MODEL=gpt-4

# For Local vLLM Server
# API_BASE_URL=http://localhost:8080/v1
# API_KEY=dummy
# API_MODEL=openai/gpt-oss-20b

# For Ollama
# API_BASE_URL=http://localhost:11434/v1
# API_KEY=dummy
# API_MODEL=mistral

# For Together AI
# API_BASE_URL=https://api.together.xyz/v1
# API_KEY=your-together-key
# API_MODEL=mistralai/Mixtral-8x7B-Instruct-v0.1

# ==================================================
# SCENARIO 2: vLLM with GPU (OpenAI GPT-OSS 20B)
# ==================================================
# LLM_MODE=vllm
# VLLM_MODEL=openai/gpt-oss-20b
# VLLM_TRUST_REMOTE_CODE=true
# VLLM_GPU_MEMORY=0.9
# VLLM_MAX_MODEL_LEN=4096
# VLLM_TEMPERATURE=0.3
# VLLM_MAX_TOKENS=500

# Alternative vLLM models
# VLLM_MODEL=mistralai/Mistral-7B-Instruct-v0.2
# VLLM_MODEL=mistralai/Mixtral-8x7B-Instruct-v0.1
# VLLM_MODEL=meta-llama/Llama-2-7b-chat-hf

# ==================================================
# SCENARIO 3: llama.cpp with CPU
# ==================================================
# LLM_MODE=llamacpp
# LLAMACPP_MODEL_PATH=models/mistral-7b-instruct-v0.2.Q4_K_M.gguf
# LLAMACPP_THREADS=8
# LLAMACPP_CONTEXT_SIZE=4096
# LLAMACPP_BATCH_SIZE=512
# LLAMACPP_TEMPERATURE=0.3
# LLAMACPP_MAX_TOKENS=500
# LLAMACPP_USE_MMAP=true
# LLAMACPP_USE_MLOCK=false

# ==================================================
# AUTO MODE (Default - Auto-detects best option)
# ==================================================
# LLM_MODE=auto

# Service Configuration
PORT=8001
