# Ollama Configuration for GPT-OSS 20B
# Note: Requires special setup - see docs/setup/OLLAMA_GPT_OSS_SETUP.md

# Use API mode with Ollama endpoint
LLM_MODE=api

# Ollama endpoint configuration
API_BASE_URL=http://localhost:11434/v1
API_KEY=dummy
API_MODEL=gpt-oss-20b

# Model parameters
API_TEMPERATURE=0.3
API_MAX_TOKENS=500

# Increase timeout for larger model (GPT-OSS 20B is slower)
API_TIMEOUT=120

# Service Configuration
PORT=8001
HOST=0.0.0.0
DEBUG=false

# CORS Settings
ALLOWED_ORIGINS=http://localhost:3000

# Logging
LOG_LEVEL=INFO

# Performance Tuning for Large Models
# Adjust based on your hardware
OLLAMA_NUM_PARALLEL=4
OLLAMA_NUM_THREAD=8
OLLAMA_MAX_LOADED_MODELS=1

# Note: For best performance with GPT-OSS 20B, consider using vLLM mode instead
# See .env.example for vLLM configuration
