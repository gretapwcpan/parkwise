# vLLM Configuration Example
# Copy this file to .env to use vLLM as your LLM provider

# LLM Mode - set to 'api' to use vLLM's OpenAI-compatible API
LLM_MODE=api

# vLLM API endpoint (assuming vLLM is running locally on port 8002)
API_BASE_URL=http://localhost:8002/v1

# Model to use (OpenAI GPT-OSS models)
# Options:
#   - openai/gpt-oss-20b (smaller, ~16GB VRAM)
#   - openai/gpt-oss-120b (larger, ~60GB VRAM)
#   - mistralai/Mistral-7B-Instruct-v0.2 (for limited VRAM)
API_MODEL=openai/gpt-oss-20b

# API key (vLLM doesn't require a real key for local deployment)
API_KEY=dummy-key

# Service port (LLM service will run on this port)
PORT=8001

# Optional: Logging level
LOG_LEVEL=INFO
