# vLLM Local Deployment Requirements
# Based on OpenAI Cookbook: https://cookbook.openai.com/articles/gpt-oss/run-vllm

# Core vLLM package
vllm>=0.5.0

# OpenAI client for testing
openai>=1.0.0

# Optional: For better performance monitoring
prometheus-client>=0.19.0
