# vLLM Local Deployment Configuration
# Copy this file to .env and adjust as needed

# Model to use (default: openai/gpt-oss-20b)
# Options:
#   - openai/gpt-oss-20b (smaller, ~16GB VRAM required)
#   - openai/gpt-oss-120b (larger, ~60GB VRAM required)
#   - mistralai/Mistral-7B-Instruct-v0.2 (for testing with less VRAM)
VLLM_MODEL=openai/gpt-oss-20b

# Server configuration
VLLM_HOST=0.0.0.0
VLLM_PORT=8002

# API endpoint for testing
VLLM_BASE_URL=http://localhost:8002/v1

# Optional: HuggingFace token if using private models
# HF_TOKEN=your_token_here

# Optional: GPU memory utilization (0.0 to 1.0)
# GPU_MEMORY_UTILIZATION=0.9

# Optional: Maximum model length
# MAX_MODEL_LEN=4096
