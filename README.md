# ParkWise - Smart Parking Space Booking System

A real-time parking space booking application with intelligent search and navigation features.

## Quick Start

### Prerequisites
- Node.js (v14 or higher)
- pnpm (v8 or higher)

### Installation

1. Clone the repository:
```bash
git clone https://github.com/gretapwcpan/parkwise.git
cd parkwise
```

2. Install pnpm if you don't have it:
```bash
npm install -g pnpm
```

3. Install dependencies:
```bash
pnpm install
```

4. Set up environment variables:
```bash
cp packages/backend/.env.example packages/backend/.env
# Edit packages/backend/.env with your configuration
```

### Running the Application

Start both backend and frontend:
```bash
pnpm run dev
```

The application will be available at:
- **Frontend**: http://localhost:3000
- **Backend API**: http://localhost:5000

### Building for Production

```bash
pnpm run build
```

## Project Structure

```
parkwise/
â”œâ”€â”€ packages/
â”‚   â”œâ”€â”€ backend/          # Express.js API server
â”‚   â”œâ”€â”€ frontend/         # React application
â”‚   â”œâ”€â”€ llm-service/      # Python LLM service (optional)
â”‚   â””â”€â”€ mcp-server/       # MCP tools server (optional)
â”œâ”€â”€ vllm-deployment/      # Local vLLM setup for testing
â””â”€â”€ docs/                 # Additional documentation
```

## Features

- ğŸ—ºï¸ Real-time parking space search
- ğŸ“ Interactive map with OpenStreetMap
- ğŸ¯ Smart location-based recommendations
- ğŸš— Turn-by-turn navigation
- ğŸ¤ Voice assistant integration
- ğŸ“± Responsive design
- ğŸ¤– Multiple LLM provider support (OpenAI, Ollama, vLLM)

## LLM Provider Setup (Optional)

ParkWise supports multiple LLM providers for intelligent search and recommendations. Choose the one that best fits your needs:

### Option 1: OpenAI API (Cloud)
Simple cloud-based solution, no local setup required:
```bash
# In packages/llm-service/.env
LLM_MODE=api
API_BASE_URL=https://api.openai.com/v1
API_KEY=your-openai-api-key
API_MODEL=gpt-3.5-turbo
```

### Option 2: Ollama (Local, CPU-friendly)
Good for local development without GPU:
```bash
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Pull a model
ollama pull mistral

# Configure llm-service
# In packages/llm-service/.env
LLM_MODE=api
API_BASE_URL=http://localhost:11434/v1
API_MODEL=mistral
```

### Option 3: vLLM (Local, GPU-optimized)

Best performance for local testing with GPU. Supports OpenAI GPT-OSS models.

#### Quick Setup

1. **Install vLLM**:
```bash
cd vllm-deployment
pip install -r requirements.txt
```

2. **Start vLLM server** (runs on port 8002):
```bash
./start.sh
# Server will be available at http://localhost:8002
```

3. **Configure LLM service** to use vLLM:
```bash
# In packages/llm-service/.env
LLM_MODE=api
API_BASE_URL=http://localhost:8002/v1
API_MODEL=openai/gpt-oss-20b
API_KEY=dummy-key
```

4. **Start the LLM service** (runs on port 8001):
```bash
cd packages/llm-service
python app.py
```

5. **Verify the connection**:
```bash
# Test vLLM directly
cd vllm-deployment
python test.py

# Test through LLM service
curl http://localhost:8001/health
```

#### Service Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend  â”‚â”€â”€â”€â”€â–¶â”‚   Backend   â”‚â”€â”€â”€â”€â–¶â”‚  LLM Service â”‚â”€â”€â”€â”€â–¶â”‚    vLLM     â”‚
â”‚   (3000)    â”‚     â”‚   (3001)    â”‚     â”‚    (8001)    â”‚     â”‚   (8002)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Supported Models

- **openai/gpt-oss-20b** - Smaller model, ~16GB VRAM
- **mistralai/Mistral-7B-Instruct-v0.2** - For limited VRAM

For more details, see [vllm-deployment/README.md](vllm-deployment/README.md).

### Comparison of LLM Providers

| Provider | Cost | Setup | Performance | Best For |
|----------|------|-------|-------------|----------|
| OpenAI API | Pay-per-use | Easy | Fast (cloud) | Production |
| Ollama | Free | Easy | Moderate (CPU) | Development |
| vLLM | Free | Moderate | Fast (GPU) | Testing with GPU |

### Complete System Startup

To run the entire system with vLLM:

```bash
# Terminal 1: Start vLLM
cd vllm-deployment
./start.sh

# Terminal 2: Start LLM service
cd packages/llm-service
python app.py

# Terminal 3: Start backend and frontend
pnpm run dev
```

The services will be available at:
- **Frontend**: http://localhost:3000
- **Backend**: http://localhost:3001
- **LLM Service**: http://localhost:8001
- **vLLM API**: http://localhost:8002

## Testing

### Running Tests

#### Backend Tests
```bash
# Run from root directory
pnpm --filter backend test

# Or navigate to backend
cd packages/backend
pnpm test

# Run specific test file
pnpm test tests/simple.test.js

# Run with coverage
pnpm test -- --coverage

# Watch mode for development
pnpm test -- --watch
```

#### Frontend Tests
```bash
# Run from root directory
pnpm --filter frontend test

# Or navigate to frontend
cd packages/frontend
pnpm test

# Run tests once (CI mode)
CI=true pnpm test -- --watchAll=false

# Run with coverage
pnpm test -- --coverage --watchAll=false
```

#### Python LLM Service Tests
```bash
cd packages/llm-service

# Install test dependencies
pip install pytest pytest-asyncio pytest-cov

# Run all tests
pytest

# Run with coverage
pytest --cov=src

# Run specific test file
pytest tests/unit/test_query_parser.py
```

### Test Structure

```
packages/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ jest.config.js           # Jest configuration
â”‚   â””â”€â”€ tests/
â”‚       â”œâ”€â”€ setup.js             # Test setup and utilities
â”‚       â”œâ”€â”€ simple.test.js       # Basic tests (working example)
â”‚       â””â”€â”€ unit/
â”‚           â””â”€â”€ services/        # Service unit tests
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ setupTests.js        # React test setup
â”‚   â”‚   â””â”€â”€ components/
â”‚   â”‚       â””â”€â”€ __tests__/       # Component tests
â””â”€â”€ llm-service/
    â”œâ”€â”€ pytest.ini               # Pytest configuration
    â””â”€â”€ tests/
        â”œâ”€â”€ conftest.py          # Shared fixtures
        â””â”€â”€ unit/                # Unit tests
```

## Troubleshooting

### Port Already in Use

If you see an error about ports being in use, find and stop the existing process:

```bash
# Find process on port 3000 or 5000
lsof -i :3000
lsof -i :5000

# Stop the process
kill -9 <PID>
```

### Dependencies Issues

If you encounter module errors, clean and reinstall:

```bash
# Remove all node_modules
rm -rf node_modules packages/*/node_modules

# Clear cache and reinstall
pnpm store prune
pnpm install
```

### Package Manager Notes

This project uses **pnpm** for package management. Key commands:
- `pnpm install` - Install all dependencies
- `pnpm add <package>` - Add a new package
- `pnpm --filter <workspace> add <package>` - Add package to specific workspace
- `pnpm run <script>` - Run a script
- `pnpm -r test` - Run tests in all packages

## License

Apache-2.0

## Support

For issues or questions, please check our [GitHub Issues](https://github.com/gretapwcpan/parkwise/issues).
